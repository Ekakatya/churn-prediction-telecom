Here is the README.md file in English, with separate sections for the main techniques and tools used, based on the analysis of your notebook.üì° Customer Churn Prediction for "TeleDom"üìÑ Project DescriptionThis project aims to predict customer churn for the telecommunications operator "TeleDom." In a highly competitive market, the marketing department seeks to identify customers who are likely to terminate their contracts. The goal is to proactively offer them special conditions and promo codes to retain them.Business Objective: Develop a machine learning model to predict whether a subscriber will terminate their contract.Target Metric: ROC-AUC.üíæ Data OverviewThe data is valid as of February 1, 2020. The dataset consists of four files joined by customerID:contract_new.csv: Contract details (payment method, duration, monthly/total charges).Target Variable: Derived from EndDate (if a termination date exists, the customer has churned).personal_new.csv: Client demographics (gender, partner status, dependents, senior citizen status).internet_new.csv: Internet service details (connection type, security, backup, etc.).phone_new.csv: Telephony service details.üîß Tools UsedThe project is implemented in Python using the following libraries:Data Manipulation: pandas, numpyVisualization: matplotlib, seabornMachine Learning: scikit-learn, xgboostStatistical Analysis: statsmodels, phikImbalanced Data: imbalanced-learnModel Interpretation: shapüß† Main TechniquesThe project employs specific advanced techniques for data processing and modeling:Correlation Analysis: Using $\phi_k$ (Phik) correlation matrix to capture non-linear relationships between categorical and numerical variables.Class Imbalance Handling: Utilizing ADASYN (Adaptive Synthetic Sampling) to generate synthetic data for the minority class (churned customers).Feature Engineering:Creating new features like agr_duration (contract duration in days).Imputing missing values based on service logic (e.g., missing internet services treated as 'No').Data Transformation Pipelines:One-Hot Encoding (OHE) for nominal categorical features.Ordinal Encoding for ordinal features.Scaling using StandardScaler and RobustScaler.Model Selection & Tuning:Comparing Logistic Regression, Random Forest, and XGBoost.Hyperparameter optimization using GridSearchCV.Model Explainability: Using SHAP (SHapley Additive exPlanations) values to interpret model predictions and understand feature importance.‚öôÔ∏è Project WorkflowData Loading & Inspection: Merging multiple datasets and correcting data types.Preprocessing: Handling missing values, removing duplicates, and feature engineering.Exploratory Data Analysis (EDA): Visualizing churn distributions and analyzing feature correlations.Preparation for Training: Splitting data, encoding features, and applying ADASYN.Modeling: Training multiple classifiers and tuning hyperparameters via cross-validation.Evaluation: Testing the best model on a hold-out set using the ROC-AUC metric.
